{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Attention-based Seq2Seq neural network for end-to-end ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the phoneme labels, paths, hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phn_61 = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay', 'b', 'bcl', 'ch', 'd', 'dcl', 'dh', 'dx', 'eh', 'el', 'em', 'en', 'eng', 'epi', 'er', 'ey', 'f', 'g', 'gcl', 'h#', 'hh', 'hv', 'ih', 'ix', 'iy', 'jh', 'k', 'kcl', 'l', 'm', 'n', 'ng', 'nx', 'ow', 'oy', 'p', 'pau', 'pcl', 'q', 'r', 's', 'sh', 't', 'tcl', 'th', 'uh', 'uw', 'ux', 'v', 'w', 'y', 'z', 'zh']\n",
    "phn_39 = ['ae', 'ao', 'aw', 'ax', 'ay', 'b', 'ch', 'd', 'dh', 'dx', 'eh', 'er', 'ey', 'f', 'g', 'h#', 'hh', 'ix', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'r', 's', 't', 'th', 'uh', 'uw', 'v', 'w', 'y', 'z', 'zh']\n",
    "mapping = {'ah': 'ax', 'ax-h': 'ax', 'ux': 'uw', 'aa': 'ao', 'ih': 'ix', 'axr': 'er', 'el': 'l', 'em': 'm', 'en': 'n', 'nx': 'n', 'eng': 'ng', 'sh': 'zh', 'hv': 'hh', 'bcl': 'h#', 'pcl': 'h#', 'dcl': 'h#', 'tcl': 'h#', 'gcl': 'h#', 'kcl': 'h#', 'q': 'h#', 'epi': 'h#', 'pau': 'h#'}\n",
    "\n",
    "TRAIN_FILE = './data/fbank/train.tfrecords'\n",
    "DEV_FILE = './data/fbank/dev.tfrecords'\n",
    "TEST_FILE = './data/fbank/test.tfrecords'\n",
    "checkpoints_path = './model/ckpt'\n",
    "\n",
    "feat_type = 'fbank'\n",
    "feats_dim = 39 if feat_type=='mfcc' else 123\n",
    "labels_sos_id = len(phn_61) + 1\n",
    "labels_eos_id = len(phn_61) \n",
    "num_classes = len(phn_61) + 1\n",
    "\n",
    "num_encoder = 256\n",
    "num_decoder = 256\n",
    "keep_prob = 0.5\n",
    "n_encoder_layer = 3\n",
    "beam_width = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import numpy as np\n",
    "import os, time\n",
    "from tensorflow.python.ops import rnn_cell_impl\n",
    "from tensorflow.python.util import nest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modify existing tensorflow code\n",
    "- apply dropout to attentional layer\n",
    "- apply tanh activation to attentional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _compute_attention(attention_mechanism, cell_output, previous_alignments,\n",
    "                       attention_layer, attention_dropout_layer, training):\n",
    "    \"\"\"Computes the attention and alignments for a given attention_mechanism.\"\"\"\n",
    "    alignments = attention_mechanism(cell_output, previous_alignments=previous_alignments)\n",
    "\n",
    "    expanded_alignments = tf.expand_dims(alignments, 1)\n",
    "    context = tf.matmul(expanded_alignments, attention_mechanism.values)\n",
    "    context = tf.squeeze(context, [1])\n",
    "\n",
    "    if attention_layer is not None:\n",
    "        attention = attention_dropout_layer(attention_layer(tf.concat([cell_output, context], 1)), training=training)\n",
    "    else:\n",
    "        attention = context\n",
    "\n",
    "    return attention, alignments    \n",
    "    \n",
    "class MyAttentionWrapper(tf.contrib.seq2seq.AttentionWrapper):\n",
    "    \n",
    "    def __init__(self, cell, attention_mechanism, keep_prob, training, attention_layer_size=None, alignment_history=False, cell_input_fn=None,\n",
    "                output_attention=True, initial_cell_state=None, name=None):\n",
    "        super(MyAttentionWrapper, self).__init__(cell, attention_mechanism, attention_layer_size, alignment_history, cell_input_fn,\n",
    "                output_attention, initial_cell_state, name)\n",
    "        \n",
    "        self.keep_prob = keep_prob\n",
    "        self.training = training\n",
    "        \n",
    "        super(tf.contrib.seq2seq.AttentionWrapper, self).__init__(name=name)\n",
    "        if not rnn_cell_impl._like_rnncell(cell):  # pylint: disable=protected-access\n",
    "            raise TypeError(\"cell must be an RNNCell, saw type: %s\" % type(cell).__name__)\n",
    "        if isinstance(attention_mechanism, (list, tuple)):\n",
    "            self._is_multi = True\n",
    "            attention_mechanisms = attention_mechanism\n",
    "            for attention_mechanism in attention_mechanisms:\n",
    "                if not isinstance(attention_mechanism, AttentionMechanism):\n",
    "                    raise TypeError(\"attention_mechanism must contain only instances of \"\n",
    "                                    \"AttentionMechanism, saw type: %s\" % type(attention_mechanism).__name__)\n",
    "        else:\n",
    "            self._is_multi = False\n",
    "            if not isinstance(attention_mechanism, tf.contrib.seq2seq.AttentionMechanism):\n",
    "                raise TypeError(\"attention_mechanism must be an AttentionMechanism or list of \"\n",
    "                                \"multiple AttentionMechanism instances, saw type: %s\" % type(attention_mechanism).__name__)\n",
    "            attention_mechanisms = (attention_mechanism,)\n",
    "\n",
    "        if cell_input_fn is None:\n",
    "            cell_input_fn = (lambda inputs, attention: tf.concat([inputs, attention], -1))\n",
    "        else:\n",
    "            if not callable(cell_input_fn):\n",
    "                raise TypeError(\"cell_input_fn must be callable, saw type: %s\" % type(cell_input_fn).__name__)\n",
    "\n",
    "        if attention_layer_size is not None:\n",
    "            attention_layer_sizes = tuple(attention_layer_size if isinstance(attention_layer_size, (list, tuple))\n",
    "                                              else (attention_layer_size,))\n",
    "            if len(attention_layer_sizes) != len(attention_mechanisms):\n",
    "                raise ValueError(\"If provided, attention_layer_size must contain exactly one \"\n",
    "                                \"integer per attention_mechanism, saw: %d vs %d\" % (len(attention_layer_sizes), len(attention_mechanisms)))\n",
    "            self._attention_layers = tuple(layers_core.Dense(attention_layer_size, name=\"attention_layer\", use_bias=True,\n",
    "                    activation=tf.tanh) for attention_layer_size in attention_layer_sizes)\n",
    "            self._attention_dropout_layers = tuple(layers_core.Dropout(rate=1-self.keep_prob, name=\"attention_dropout_layer\")\n",
    "                                                  for attention_layer_size in attention_layer_sizes)\n",
    "            self._attention_layer_size = sum(attention_layer_sizes)\n",
    "        else:\n",
    "            self._attention_layers = None\n",
    "            self._attention_dropout_layers = None\n",
    "            self._attention_layer_size = sum(attention_mechanism.values.get_shape()[-1].value\n",
    "                                                      for attention_mechanism in attention_mechanisms)\n",
    "            \n",
    "        self._cell = cell\n",
    "        self._attention_mechanisms = attention_mechanisms\n",
    "        self._cell_input_fn = cell_input_fn\n",
    "        self._output_attention = output_attention\n",
    "        self._alignment_history = alignment_history\n",
    "        with tf.name_scope(name, \"AttentionWrapperInit\"):\n",
    "            if initial_cell_state is None:\n",
    "                self._initial_cell_state = None\n",
    "            else:\n",
    "                final_state_tensor = nest.flatten(initial_cell_state)[-1]\n",
    "                state_batch_size = (final_state_tensor.shape[0].value or tf.shape(final_state_tensor)[0])\n",
    "                error_message = (\n",
    "                \"When constructing AttentionWrapper %s: \" % self._base_name +\n",
    "                \"Non-matching batch sizes between the memory \"\n",
    "                \"(encoder output) and initial_cell_state.  Are you using \"\n",
    "                \"the BeamSearchDecoder?  You may need to tile your initial state \"\n",
    "                \"via the tf.contrib.seq2seq.tile_batch function with argument \"\n",
    "                \"multiple=beam_width.\")\n",
    "                with tf.control_dependencies(\n",
    "                    self._batch_size_checks(state_batch_size, error_message)):\n",
    "                    self._initial_cell_state = nest.map_structure(lambda s: tf.identity(s, name=\"check_initial_cell_state\"),\n",
    "                                                              initial_cell_state)\n",
    "                    \n",
    "    def call(self, inputs, state):\n",
    "\n",
    "        if not isinstance(state, tf.contrib.seq2seq.AttentionWrapperState):\n",
    "            raise TypeError(\"Expected state to be instance of MyAttentionWrapperState. \"\n",
    "                          \"Received type %s instead.\"  % type(state))\n",
    "        \n",
    "        cell_inputs = self._cell_input_fn(inputs, state.attention)\n",
    "        cell_state = state.cell_state\n",
    "        cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\n",
    "\n",
    "        cell_batch_size = (\n",
    "            cell_output.shape[0].value or tf.shape(cell_output)[0])\n",
    "        error_message = (\n",
    "            \"When applying AttentionWrapper %s: \" % self.name +\n",
    "            \"Non-matching batch sizes between the memory \"\n",
    "            \"(encoder output) and the query (decoder output).  Are you using \"\n",
    "            \"the BeamSearchDecoder?  You may need to tile your memory input via \"\n",
    "            \"the tf.contrib.seq2seq.tile_batch function with argument \"\n",
    "            \"multiple=beam_width.\")\n",
    "        with tf.control_dependencies(\n",
    "            self._batch_size_checks(cell_batch_size, error_message)):\n",
    "            cell_output = tf.identity(\n",
    "              cell_output, name=\"checked_cell_output\")\n",
    "\n",
    "        if self._is_multi:\n",
    "            previous_alignments = state.alignments\n",
    "            previous_alignment_history = state.alignment_history\n",
    "        else:\n",
    "            previous_alignments = [state.alignments]\n",
    "            previous_alignment_history = [state.alignment_history]\n",
    "\n",
    "        all_alignments = []\n",
    "        all_attentions = []\n",
    "        all_histories = []\n",
    "        for i, attention_mechanism in enumerate(self._attention_mechanisms):\n",
    "            attention, alignments = _compute_attention(\n",
    "            attention_mechanism, cell_output, previous_alignments[i],\n",
    "            self._attention_layers[i] if self._attention_layers else None,\n",
    "            self._attention_dropout_layers[i] if self._attention_dropout_layers else None,\n",
    "            self.training)\n",
    "            alignment_history = previous_alignment_history[i].write(\n",
    "            state.time, alignments) if self._alignment_history else ()\n",
    "\n",
    "            all_alignments.append(alignments)\n",
    "            all_histories.append(alignment_history)\n",
    "            all_attentions.append(attention)\n",
    "\n",
    "        attention = tf.concat(all_attentions, 1)\n",
    "        \n",
    "        next_state = tf.contrib.seq2seq.AttentionWrapperState(\n",
    "            time=state.time + 1,\n",
    "            cell_state=next_cell_state,\n",
    "            attention=attention,\n",
    "            alignments=self._item_or_tuple(all_alignments),\n",
    "            alignment_history=self._item_or_tuple(all_histories))\n",
    "        \n",
    "        if self._output_attention:\n",
    "            return attention, next_state\n",
    "        else:\n",
    "            return cell_output, next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, data_path, model_type):\n",
    "        self.data_path = data_path\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self._get_data()\n",
    "        self._build()\n",
    "    \n",
    "    def _get_data(self):\n",
    "        iterator = self._get_iterator()\n",
    "        self.iterator_initializer = iterator.initializer\n",
    "        \n",
    "        batched_data = iterator.get_next()\n",
    "        self.features = batched_data[0]\n",
    "        self.labels_in = batched_data[1]\n",
    "        self.labels_out = batched_data[2]\n",
    "        self.feats_seq_len = tf.to_int32(batched_data[3])\n",
    "        self.labels_in_seq_len = tf.to_int32(batched_data[4])\n",
    "        \n",
    "    def _build(self):\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.training = tf.placeholder(tf.bool)\n",
    "    \n",
    "        decoded = self._compute_dynamic_decode()\n",
    "        self.saver = tf.train.Saver(max_to_keep=5)\n",
    "        \n",
    "        if self.model_type == 'train':\n",
    "            self.logits = decoded\n",
    "            self.loss = self._compute_loss()\n",
    "            self.update_step = self._get_update_step()\n",
    "        elif self.model_type == 'eval':\n",
    "            self.logits = decoded\n",
    "            self.loss = self._compute_loss()\n",
    "        elif self.model_type == 'infer':\n",
    "            self.predicted_ids = decoded\n",
    "            self.per = self._compute_per() # tuple of unnormalized edit distance and seqeucne_length\n",
    "        else:\n",
    "            raise Exception('invalid model_type')\n",
    "        \n",
    "    def _get_iterator(self):\n",
    "        dataset = tf.contrib.data.TFRecordDataset(self.data_path)\n",
    "        context_features = {'feats_seq_len': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                           'labels_seq_len': tf.FixedLenFeature([], dtype=tf.int64)}\n",
    "        sequence_features = {'features': tf.FixedLenSequenceFeature([feats_dim], dtype=tf.float32),\n",
    "                            'labels': tf.FixedLenSequenceFeature([], dtype=tf.int64)}\n",
    "        dataset = dataset.map(lambda serialized_example: tf.parse_single_sequence_example(serialized_example,\n",
    "                                                                                         context_features=context_features,\n",
    "                                                                                         sequence_features=sequence_features))\n",
    "        dataset = dataset.map(lambda context, sequence: (sequence['features'], sequence['labels'], \n",
    "                                                         context['feats_seq_len'], context['labels_seq_len']))\n",
    "        dataset = dataset.map(lambda features, labels, feats_seq_len, labels_seq_len: (features, \n",
    "                                                        tf.concat(([labels_sos_id], labels),0),\n",
    "                                                        tf.concat((labels, [labels_eos_id]), 0),\n",
    "                                                                feats_seq_len, labels_seq_len))\n",
    "        dataset = dataset.map(lambda features, labels_in, labels_out, feats_seq_len, labels_seq_len: \n",
    "                                          (features, labels_in, labels_out, feats_seq_len, tf.size(labels_in, out_type=tf.int64)))\n",
    "        def batching_func(x):\n",
    "            return x.padded_batch(batch_size,\n",
    "                                 padded_shapes=(tf.TensorShape([None, feats_dim]),\n",
    "                                               tf.TensorShape([None]),\n",
    "                                               tf.TensorShape([None]),\n",
    "                                               tf.TensorShape([]),\n",
    "                                               tf.TensorShape([])),\n",
    "                                 padding_values=(tf.cast(0, tf.float32),\n",
    "                                                tf.cast(labels_eos_id, tf.int64),\n",
    "                                                tf.cast(labels_eos_id, tf.int64),\n",
    "                                                tf.cast(0, tf.int64),\n",
    "                                                tf.cast(0, tf.int64)))\n",
    "        \n",
    "        def key_func(features, labels_in, labels_out, feats_seq_len, labels_in_seq_len):\n",
    "            f0 = lambda: tf.constant(0, tf.int64)\n",
    "            f1 = lambda: tf.constant(1, tf.int64)\n",
    "            f2 = lambda: tf.constant(2, tf.int64)\n",
    "            f3 = lambda: tf.constant(3, tf.int64)\n",
    "            f4 = lambda: tf.constant(4, tf.int64)\n",
    "            f5 = lambda: tf.constant(5, tf.int64)\n",
    "            f6 = lambda: tf.constant(6, tf.int64)\n",
    "            \n",
    "            return tf.case([(tf.less_equal(feats_seq_len, 200), f0),\n",
    "                   (tf.less_equal(feats_seq_len, 250), f1),\n",
    "                   (tf.less_equal(feats_seq_len, 300), f2),\n",
    "                   (tf.less_equal(feats_seq_len, 350), f3),\n",
    "                   (tf.less_equal(feats_seq_len, 400), f4),\n",
    "                   (tf.less_equal(feats_seq_len, 500), f5)], default=f6)\n",
    "        \n",
    "        def reduce_func(bucket_id, windowed_data):\n",
    "            return batching_func(windowed_data)\n",
    "        \n",
    "        if self.model_type=='train':\n",
    "            self.shuffle_seed = tf.placeholder(tf.int64)\n",
    "            dataset = dataset.shuffle(10000, seed=self.shuffle_seed)\n",
    "            batched_dataset = dataset.group_by_window(key_func=key_func, reduce_func=reduce_func, window_size=batch_size)\n",
    "            batched_dataset = batched_dataset.shuffle(10000, seed=-self.shuffle_seed)\n",
    "        else: # eval or infer, no shuffle\n",
    "            batched_dataset = batching_func(dataset)\n",
    "            \n",
    "        return batched_dataset.make_initializable_iterator()\n",
    "    \n",
    "    def _compute_encoder_outputs(self):\n",
    "        def residual_block(inp, out_channels):\n",
    "            inp_channels = inp.get_shape().as_list()[3]\n",
    "            out = tf.layers.conv2d(inp, filters=out_channels, kernel_size=(3,3), strides=(1,1), padding='same', use_bias=False)\n",
    "            out = tf.layers.batch_normalization(out, training=self.training) \n",
    "            out = tf.nn.relu(out)\n",
    "            out = tf.layers.dropout(out, rate=1-self.keep_prob, training=self.training)\n",
    "            out = tf.layers.conv2d(out, filters=out_channels, kernel_size=(3,3), strides=(1,1), padding='same', use_bias=False)\n",
    "            out = tf.layers.batch_normalization(out, training=self.training)\n",
    "            out = tf.nn.relu(out)\n",
    "            out = tf.layers.dropout(out, rate=1-self.keep_prob, training=self.training)\n",
    "            if inp_channels != out_channels:\n",
    "                inp = tf.layers.conv2d(inp, filters=out_channels, kernel_size=(1,1), strides=(1,1), padding='same')\n",
    "            return out + inp\n",
    "        def conv_block(inp, filters, kernel_size, strides):\n",
    "            out = tf.layers.conv2d(inp, filters=filters, kernel_size=kernel_size, strides=strides, padding='same', use_bias=False)\n",
    "            out = tf.layers.batch_normalization(out, training=self.training) \n",
    "            out = tf.nn.relu(out)\n",
    "            out = tf.layers.dropout(out, rate=1-self.keep_prob, training=self.training)\n",
    "            return out\n",
    "        def dense_block(inp, num_units):\n",
    "            out = tf.layers.dense(inp, num_units, use_bias=False)\n",
    "            out = tf.layers.batch_normalization(out, training=self.training) \n",
    "            out = tf.nn.relu(out)\n",
    "            out = tf.layers.dropout(out, rate=1-self.keep_prob, training=self.training)\n",
    "            return out\n",
    "         \n",
    "        features = tf.stack(tf.split(self.features, num_or_size_splits=3, axis=2), axis=3)\n",
    "        features = tf.transpose(features, [0,2,1,3]) # shape = [batch, feats_dim/3, max_time, channels]\n",
    "        \n",
    "        conv = conv_block(features, 128, (3,3), (1,3)) # time -> time/3\n",
    "        conv = residual_block(conv, 64)\n",
    "        conv = residual_block(conv, 64)\n",
    "        conv = residual_block(conv, 64)\n",
    "        \n",
    "        flattend = tf.transpose(conv, [0,2,1,3])\n",
    "        flattend = tf.reshape(flattend, [tf.shape(flattend)[0], tf.shape(flattend)[1], 41*64])\n",
    "        \n",
    "        fc = dense_block(flattend, 1024)\n",
    "        \n",
    "        cells_fw = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_encoder, use_peepholes=True, initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)),\n",
    "                                                  output_keep_prob=self.keep_prob) for _ in range(n_encoder_layer)]\n",
    "        cells_bw = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_encoder, use_peepholes=True, initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)),\n",
    "                                                  output_keep_prob=self.keep_prob) for _ in range(n_encoder_layer)]\n",
    "        \n",
    "        outputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, fc,\n",
    "                                                            dtype=tf.float32, sequence_length=self.feats_seq_len//3)\n",
    "        return outputs, self.feats_seq_len//3\n",
    "        \n",
    "    def _get_decoder_cell_and_init_state(self):    \n",
    "        num_batch = tf.shape(self.memory)[0]\n",
    "        decoder_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_decoder, use_peepholes=True, initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)),\n",
    "                                                     output_keep_prob=self.keep_prob)\n",
    "\n",
    "        if self.model_type == 'infer':\n",
    "            self.memory = tf.contrib.seq2seq.tile_batch(self.memory, multiplier=beam_width)\n",
    "            self.mem_seq_len = tf.contrib.seq2seq.tile_batch(self.mem_seq_len, multiplier=beam_width)\n",
    "            num_batch = num_batch * beam_width\n",
    "\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_decoder, self.memory, self.mem_seq_len, scale=False)\n",
    "        decoder_cell = MyAttentionWrapper(decoder_cell, attention_mechanism, self.keep_prob, self.training,\n",
    "                                          attention_layer_size=num_decoder, output_attention=True)\n",
    "        decoder_initial_state = decoder_cell.zero_state(num_batch, tf.float32)\n",
    "        return (decoder_cell, decoder_initial_state)\n",
    "    \n",
    "    def _compute_dynamic_decode(self):\n",
    "        embedding_decoder = tf.Variable(tf.concat([np.identity(num_classes-1), tf.zeros([2,num_classes-1])], axis=0),\n",
    "                                        dtype=tf.float32, trainable=False)\n",
    "        #embedding_decoder = tf.Variable(tf.random_uniform([num_classes+1, 30], minval=-0.05, maxval=0.05, dtype=tf.float32))\n",
    "        decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, self.labels_in)\n",
    "    \n",
    "        self.memory, self.mem_seq_len = self._compute_encoder_outputs()\n",
    "        with tf.variable_scope('decoder') as decoder_scope:\n",
    "            decoder_cell, decoder_initial_state = self._get_decoder_cell_and_init_state()\n",
    "        \n",
    "            output_layer = layers_core.Dense(num_classes, use_bias=True)\n",
    "\n",
    "            if self.model_type in ['train', 'eval']:\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, self.labels_in_seq_len, time_major=False)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state=decoder_initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=False, scope=decoder_scope)\n",
    "                return output_layer(outputs.rnn_output)\n",
    "            else: # infer\n",
    "                beam_decoder = tf.contrib.seq2seq.BeamSearchDecoder(decoder_cell, embedding_decoder, \n",
    "                                                    tf.fill([tf.shape(self.features)[0]], labels_sos_id), labels_eos_id,\n",
    "                                                            decoder_initial_state, beam_width, output_layer=output_layer)\n",
    "                decoded, _, final_seq_len = tf.contrib.seq2seq.dynamic_decode(beam_decoder, maximum_iterations=100,\n",
    "                                                                              output_time_major=False, scope=decoder_scope)\n",
    "                return decoded.predicted_ids # shape = [batch, max_time, beam_width]\n",
    "    \n",
    "    def _compute_loss(self):\n",
    "        max_time = tf.shape(self.labels_out)[1]\n",
    "        target_weights = tf.sequence_mask(self.labels_in_seq_len, max_time, dtype=self.logits.dtype)\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels_out, logits=self.logits)\n",
    "        return tf.reduce_sum(crossent * target_weights) / tf.to_float(tf.shape(self.logits)[0])\n",
    "    \n",
    "    def _get_update_step(self):\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            params = tf.trainable_variables()\n",
    "            #\n",
    "            if weight_decay is not None:\n",
    "                l2_loss = 0\n",
    "                for v in tf.trainable_variables():\n",
    "                    if 'kernel' in v.name:\n",
    "                        l2_loss += tf.nn.l2_loss(v)\n",
    "                self.loss += weight_decay * l2_loss\n",
    "            #\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "            #clipped_gradients = gradients # no clipping\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "        return update_step\n",
    "    \n",
    "    def _get_sparse_tensor(self, dense, default):\n",
    "        indices = tf.to_int64(tf.where(tf.not_equal(dense, default)))\n",
    "        vals = tf.to_int32(tf.gather_nd(dense, indices))\n",
    "        shape = tf.to_int64(tf.shape(dense))\n",
    "        return tf.SparseTensor(indices, vals, shape)\n",
    "    \n",
    "    def _compute_per(self):\n",
    "        '''\n",
    "        return tuple: (unnormalized edit distance, seqeucne_length),\n",
    "        it is just sum of batched data\n",
    "        '''\n",
    "        phn_61_tensor = tf.constant(phn_61, dtype=tf.string)\n",
    "        phn_39_tensor = tf.constant(phn_39, dtype=tf.string)\n",
    "        mapping_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(mapping.keys()), list(mapping.values())), default_value='')\n",
    "        self.mapping_table_init = mapping_table.init\n",
    "        \n",
    "        def map_to_reduced_phn(p):\n",
    "            val = mapping_table.lookup(phn_61_tensor[p])\n",
    "            f1 = lambda: tf.to_int32(tf.reduce_min(tf.where(tf.equal(val, phn_39_tensor))))\n",
    "            f2 = lambda: tf.to_int32(tf.reduce_min(tf.where(tf.equal(phn_61_tensor[p], phn_39_tensor))))\n",
    "            return tf.cond(tf.not_equal(val, ''), f1, f2)\n",
    "        \n",
    "        indices = tf.to_int64(tf.where(tf.logical_and(tf.not_equal(self.predicted_ids[:,:,0], -1), \n",
    "                                                      tf.not_equal(self.predicted_ids[:,:,0], labels_eos_id))))\n",
    "        vals = tf.to_int32(tf.gather_nd(self.predicted_ids[:,:,0], indices))\n",
    "        shape = tf.to_int64(tf.shape(self.predicted_ids[:,:,0]))\n",
    "        decoded_sparse = tf.SparseTensor(indices, vals, shape)\n",
    "        labels_out_sparse = self._get_sparse_tensor(self.labels_out, labels_eos_id)\n",
    "        \n",
    "        decoded_reduced = tf.SparseTensor(decoded_sparse.indices, tf.map_fn(map_to_reduced_phn, decoded_sparse.values), decoded_sparse.dense_shape)\n",
    "        labels_out_reduced = tf.SparseTensor(labels_out_sparse.indices, tf.map_fn(map_to_reduced_phn, labels_out_sparse.values), labels_out_sparse.dense_shape)\n",
    "        \n",
    "        return tf.reduce_sum(tf.edit_distance(decoded_reduced, labels_out_reduced, normalize=False)) , tf.to_float(tf.size(labels_out_reduced.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, \n",
      "train_loss=150.404, time=145s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-1\n",
      "\tdev_loss=138.242, time=10s\n",
      "Epoch 2/50, \n",
      "train_loss=125.022, time=136s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-2\n",
      "\tdev_loss=111.674, time=9s\n",
      "Epoch 3/50, \n",
      "train_loss=113.847, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-3\n",
      "\tdev_loss=108.540, time=9s\n",
      "Epoch 4/50, \n",
      "train_loss=111.071, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-4\n",
      "\tdev_loss=106.417, time=9s\n",
      "Epoch 5/50, \n",
      "train_loss=108.955, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-5\n",
      "\tdev_loss=103.836, time=9s\n",
      "Epoch 6/50, \n",
      "train_loss=106.841, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-6\n",
      "\tdev_loss=103.209, time=9s\n",
      "Epoch 7/50, \n",
      "train_loss=105.354, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-7\n",
      "\tdev_loss=100.979, time=8s\n",
      "Epoch 8/50, \n",
      "train_loss=103.998, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-8\n",
      "\tdev_loss=98.105, time=9s\n",
      "Epoch 9/50, \n",
      "train_loss=101.724, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-9\n",
      "\tdev_loss=95.467, time=9s\n",
      "Epoch 10/50, \n",
      "train_loss=98.664, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-10\n",
      "\tdev_loss=90.776, time=9s\n",
      "Epoch 11/50, \n",
      "train_loss=93.017, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-11\n",
      "\tdev_loss=79.365, time=9s\n",
      "Epoch 12/50, \n",
      "train_loss=85.384, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-12\n",
      "\tdev_loss=67.553, time=9s\n",
      "Epoch 13/50, \n",
      "train_loss=76.936, time=132s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-13\n",
      "\tdev_loss=59.226, time=9s\n",
      "Epoch 14/50, \n",
      "train_loss=69.347, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-14\n",
      "\tdev_loss=50.873, time=8s\n",
      "Epoch 15/50, \n",
      "train_loss=63.285, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-15\n",
      "\tdev_loss=44.842, time=9s\n",
      "Epoch 16/50, \n",
      "train_loss=58.002, time=132s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-16\n",
      "\tdev_loss=40.841, time=9s\n",
      "Epoch 17/50, \n",
      "train_loss=53.487, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-17\n",
      "\tdev_loss=37.397, time=9s\n",
      "Epoch 18/50, \n",
      "train_loss=50.103, time=132s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-18\n",
      "\tdev_loss=35.242, time=9s\n",
      "Epoch 19/50, \n",
      "train_loss=47.041, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-19\n",
      "\tdev_loss=32.560, time=9s\n",
      "Epoch 20/50, \n",
      "train_loss=44.252, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-20\n",
      "\tdev_loss=30.684, time=9s\n",
      "Epoch 21/50, \n",
      "train_loss=42.089, time=132s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-21\n",
      "\tdev_loss=30.205, time=9s\n",
      "Epoch 22/50, \n",
      "train_loss=40.323, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-22\n",
      "\tdev_loss=29.133, time=9s\n",
      "Epoch 23/50, \n",
      "train_loss=38.700, time=132s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-23\n",
      "\tdev_loss=28.179, time=9s\n",
      "Epoch 24/50, \n",
      "train_loss=36.975, time=132s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-24\n",
      "\tdev_loss=27.376, time=9s\n",
      "Epoch 25/50, \n",
      "train_loss=35.475, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-25\n",
      "\tdev_loss=26.954, time=9s\n",
      "Epoch 26/50, \n",
      "train_loss=34.252, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-26\n",
      "\tdev_loss=25.532, time=9s\n",
      "Epoch 27/50, \n",
      "train_loss=33.025, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-27\n",
      "\tdev_loss=25.384, time=9s\n",
      "Epoch 28/50, \n",
      "train_loss=32.171, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-28\n",
      "\tdev_loss=25.650, time=9s\n",
      "Epoch 29/50, \n",
      "train_loss=30.981, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-29\n",
      "\tdev_loss=24.483, time=9s\n",
      "Epoch 30/50, \n",
      "train_loss=30.208, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-30\n",
      "\tdev_loss=24.364, time=9s\n",
      "Epoch 31/50, \n",
      "train_loss=29.451, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-31\n",
      "\tdev_loss=24.487, time=8s\n",
      "Epoch 32/50, \n",
      "train_loss=28.741, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-32\n",
      "\tdev_loss=24.270, time=9s\n",
      "Epoch 33/50, \n",
      "train_loss=27.740, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-33\n",
      "\tdev_loss=23.937, time=8s\n",
      "Epoch 34/50, \n",
      "train_loss=27.202, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-34\n",
      "\tdev_loss=23.702, time=9s\n",
      "Epoch 35/50, \n",
      "train_loss=26.603, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-35\n",
      "\tdev_loss=24.183, time=9s\n",
      "Epoch 36/50, \n",
      "train_loss=25.641, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-36\n",
      "\tdev_loss=23.504, time=9s\n",
      "Epoch 37/50, \n",
      "train_loss=25.117, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-37\n",
      "\tdev_loss=23.559, time=8s\n",
      "Epoch 38/50, \n",
      "train_loss=24.917, time=135s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-38\n",
      "\tdev_loss=23.575, time=9s\n",
      "Epoch 39/50, \n",
      "train_loss=24.179, time=133s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-39\n",
      "\tdev_loss=23.299, time=9s\n",
      "Epoch 40/50, \n",
      "train_loss=23.700, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-40\n",
      "\tdev_loss=23.402, time=9s\n",
      "Epoch 41/50, \n",
      "train_loss=23.112, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-41\n",
      "\tdev_loss=23.923, time=9s\n",
      "Epoch 42/50, \n",
      "train_loss=22.681, time=135s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-42\n",
      "\tdev_loss=23.100, time=9s\n",
      "Epoch 43/50, \n",
      "train_loss=22.118, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-43\n",
      "\tdev_loss=24.039, time=9s\n",
      "Epoch 44/50, \n",
      "train_loss=21.708, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-44\n",
      "\tdev_loss=23.288, time=9s\n",
      "Epoch 45/50, \n",
      "train_loss=21.283, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-45\n",
      "\tdev_loss=24.279, time=9s\n",
      "Epoch 46/50, \n",
      "train_loss=20.954, time=134s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-46\n",
      "\tdev_loss=23.675, time=9s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-962681ba1f69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             _, loss = train_sess.run([train_model.update_step, train_model.loss], feed_dict={train_model.keep_prob: keep_prob,\n\u001b[1;32m---> 28\u001b[1;33m                                                                                              train_model.training: True})\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = None\n",
    "epochs = 50\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "dev_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    tf.set_random_seed(159)\n",
    "    train_model = Model(data_path=TRAIN_FILE, model_type='train')\n",
    "    initializer = tf.global_variables_initializer()\n",
    "\n",
    "with dev_graph.as_default():\n",
    "    dev_model = Model(data_path=DEV_FILE, model_type='eval')\n",
    "    \n",
    "train_sess = tf.Session(graph=train_graph)\n",
    "train_sess.run(initializer)\n",
    "\n",
    "dev_sess = tf.Session(graph=dev_graph)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_sess.run(train_model.iterator_initializer, feed_dict={train_model.shuffle_seed: epoch})\n",
    "    train_loss = []\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            _, loss = train_sess.run([train_model.update_step, train_model.loss], feed_dict={train_model.keep_prob: keep_prob,\n",
    "                                                                                             train_model.training: True})\n",
    "            train_loss.append(loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            end = time.time()\n",
    "            log = \"Epoch {}/{}, \\ntrain_loss={:.3f}, time={:.0f}s\"\n",
    "            print(log.format(epoch+1, epochs, np.mean(train_loss), end-start))\n",
    "            if not os.path.isdir(os.path.split(checkpoints_path)[0]):\n",
    "                os.makedirs(os.path.split(checkpoints_path)[0])\n",
    "            saved_ckpt_path = train_model.saver.save(train_sess, checkpoints_path, global_step=epoch+1)\n",
    "            \n",
    "            dev_model.saver.restore(dev_sess, saved_ckpt_path)\n",
    "            dev_sess.run(dev_model.iterator_initializer)\n",
    "            start = time.time()\n",
    "            dev_loss = []\n",
    "            while True:\n",
    "                try:\n",
    "                    loss = dev_sess.run(dev_model.loss, feed_dict={dev_model.keep_prob: 1.0, dev_model.training: False})\n",
    "                    dev_loss.append(loss)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    end = time.time()\n",
    "                    log = \"\\tdev_loss={:.3f}, time={:.0f}s\"\n",
    "                    print(log.format(np.mean(dev_loss), end-start))\n",
    "                    break\n",
    "            \n",
    "            # go to next training epoch\n",
    "            break\n",
    "            \n",
    "train_sess.close()\n",
    "dev_sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tunning stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-46\n",
      "Epoch 1/15, \n",
      "train_loss=19.550, time=137s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-1\n",
      "\tdev_loss=23.258, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-1\n",
      "\t\ttest_per=0.162, time=13s\n",
      "Epoch 2/15, \n",
      "train_loss=18.846, time=137s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-2\n",
      "\tdev_loss=23.238, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-2\n",
      "\t\ttest_per=0.162, time=12s\n",
      "Epoch 3/15, \n",
      "train_loss=18.560, time=139s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-3\n",
      "\tdev_loss=23.272, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-3\n",
      "\t\ttest_per=0.163, time=12s\n",
      "Epoch 4/15, \n",
      "train_loss=18.233, time=137s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-4\n",
      "\tdev_loss=23.388, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-4\n",
      "\t\ttest_per=0.164, time=12s\n",
      "Epoch 5/15, \n",
      "train_loss=18.014, time=138s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-5\n",
      "\tdev_loss=23.247, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-5\n",
      "\t\ttest_per=0.161, time=12s\n",
      "Epoch 6/15, \n",
      "train_loss=17.734, time=138s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-6\n",
      "\tdev_loss=23.377, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-6\n",
      "\t\ttest_per=0.160, time=12s\n",
      "Epoch 7/15, \n",
      "train_loss=17.560, time=138s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-7\n",
      "\tdev_loss=23.511, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-7\n",
      "\t\ttest_per=0.160, time=12s\n",
      "Epoch 8/15, \n",
      "train_loss=17.720, time=139s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-8\n",
      "\tdev_loss=23.202, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-8\n",
      "\t\ttest_per=0.160, time=12s\n",
      "Epoch 9/15, \n",
      "train_loss=17.202, time=139s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-9\n",
      "\tdev_loss=23.460, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-9\n",
      "\t\ttest_per=0.160, time=12s\n",
      "Epoch 10/15, \n",
      "train_loss=17.337, time=138s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-10\n",
      "\tdev_loss=23.212, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-10\n",
      "\t\ttest_per=0.161, time=12s\n",
      "Epoch 11/15, \n",
      "train_loss=16.990, time=139s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-11\n",
      "\tdev_loss=23.641, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-11\n",
      "\t\ttest_per=0.160, time=12s\n",
      "Epoch 12/15, \n",
      "train_loss=16.777, time=138s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-12\n",
      "\tdev_loss=23.493, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-12\n",
      "\t\ttest_per=0.160, time=12s\n",
      "Epoch 13/15, \n",
      "train_loss=16.515, time=139s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-13\n",
      "\tdev_loss=23.572, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-13\n",
      "\t\ttest_per=0.159, time=12s\n",
      "Epoch 14/15, \n",
      "train_loss=16.471, time=140s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-14\n",
      "\tdev_loss=23.539, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-14\n",
      "\t\ttest_per=0.158, time=12s\n",
      "Epoch 15/15, \n",
      "train_loss=16.212, time=139s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-15\n",
      "\tdev_loss=23.759, time=9s\n",
      "INFO:tensorflow:Restoring parameters from ./model/ckpt-15\n",
      "\t\ttest_per=0.160, time=12s\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001 # finetuning learning rate\n",
    "weight_decay = 0.00001\n",
    "epochs = 15\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "dev_graph = tf.Graph()\n",
    "test_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    tf.set_random_seed(159)\n",
    "    train_model = Model(data_path=TRAIN_FILE, model_type='train')\n",
    "    initializer = tf.global_variables_initializer()\n",
    "\n",
    "with dev_graph.as_default():\n",
    "    dev_model = Model(data_path=DEV_FILE, model_type='eval')\n",
    "with test_graph.as_default():\n",
    "    test_model = Model(data_path=TEST_FILE, model_type='infer')\n",
    "    \n",
    "train_sess = tf.Session(graph=train_graph)\n",
    "train_sess.run(initializer)\n",
    "train_model.saver.restore(train_sess, './model/ckpt-46')\n",
    "\n",
    "dev_sess = tf.Session(graph=dev_graph)\n",
    "\n",
    "test_sess = tf.Session(graph=test_graph)\n",
    "test_sess.run(test_model.mapping_table_init)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_sess.run(train_model.iterator_initializer, feed_dict={train_model.shuffle_seed: epoch})\n",
    "    train_loss = []\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            _, loss = train_sess.run([train_model.update_step, train_model.loss], feed_dict={train_model.keep_prob: keep_prob,\n",
    "                                                                                             train_model.training: True})\n",
    "            train_loss.append(loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            end = time.time()\n",
    "            log = \"Epoch {}/{}, \\ntrain_loss={:.3f}, time={:.0f}s\"\n",
    "            print(log.format(epoch+1, epochs, np.mean(train_loss), end-start))\n",
    "            if not os.path.isdir(checkpoints_path):\n",
    "                os.makedirs(checkpoints_path)\n",
    "            saved_ckpt_path = train_model.saver.save(train_sess, checkpoints_path, global_step=epoch+1)\n",
    "            \n",
    "            dev_model.saver.restore(dev_sess, saved_ckpt_path)\n",
    "            dev_sess.run(dev_model.iterator_initializer)\n",
    "            start = time.time()\n",
    "            dev_loss = []\n",
    "            while True:\n",
    "                try:\n",
    "                    loss = dev_sess.run(dev_model.loss, feed_dict={dev_model.keep_prob: 1.0, dev_model.training: False})\n",
    "                    dev_loss.append(loss)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    end = time.time()\n",
    "                    log = \"\\tdev_loss={:.3f}, time={:.0f}s\"\n",
    "                    print(log.format(np.mean(dev_loss), end-start))\n",
    "                    break\n",
    "            \n",
    "            test_model.saver.restore(test_sess, saved_ckpt_path)\n",
    "            test_sess.run(test_model.iterator_initializer)\n",
    "            test_unnormed_edit_dist = []\n",
    "            test_seq_len = []\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                try:\n",
    "                    unnormed_edit_dist, seq_len = test_sess.run(test_model.per, feed_dict={test_model.keep_prob: 1.0,\n",
    "                                                                                           test_model.training: False})\n",
    "                    test_unnormed_edit_dist.append(unnormed_edit_dist)\n",
    "                    test_seq_len.append(seq_len)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    end = time.time()\n",
    "                    log = '\\t\\ttest_per={:.3f}, time={:.0f}s'\n",
    "                    print(log.format(sum(test_unnormed_edit_dist)/sum(test_seq_len), end-start))\n",
    "                    break\n",
    "            # go to next training epoch\n",
    "            break\n",
    "            \n",
    "train_sess.close()\n",
    "dev_sess.close()\n",
    "test_sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
